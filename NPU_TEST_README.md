# NPU 算子测试说明

这个测试套件用于验证 Liger-Kernel 的主要算子在 NPU 上是否能正常运行。

## 测试的算子

✅ 共测试 9 个核心算子，每个都包含 forward 和 backward 测试：

1. **SwiGLU** - SiLU 激活函数的门控版本
2. **GeGLU** - GELU 激活函数的门控版本  
3. **RMSNorm** - Root Mean Square 归一化
4. **LayerNorm** - 层归一化
5. **RoPE** - 旋转位置编码
6. **Softmax** - Softmax 激活
7. **CrossEntropy** - 交叉熵损失
8. **KL Divergence** - KL 散度损失
9. **JSD** - Jensen-Shannon 散度

## 使用方法

### 基本运行

```bash
# 在项目根目录下运行
python test_npu_ops.py
```

### 修改设备

如果你的 NPU 设备名不是 `"npu"`，请修改文件开头的 `DEVICE` 变量：

```python
# 在 test_npu_ops.py 第 9 行
DEVICE = "npu"  # 改成你的设备名，如 "npu:0", "ascend" 等
```

### 修改数据类型

默认使用 `float32`，如需测试其他精度：

```python
# 在 test_npu_ops.py 第 10 行
DTYPE = torch.float32  # 可改为 torch.float16 或 torch.bfloat16
```

### 修改容差

根据你的精度需求调整误差容差：

```python
# 在 test_npu_ops.py 第 11 行
TOLERANCE = 1e-5  # float32 推荐 1e-5, float16 推荐 1e-3
```

## 输出示例

```
======================================================================
NPU 算子测试开始
设备: npu
数据类型: torch.float32
容差: 1e-05
======================================================================

======================================================================
测试: SwiGLU
======================================================================
输入形状: (2, 4, 128)
✓ SwiGLU Forward: 误差=1.192093e-07
✓ SwiGLU Backward: 误差=2.384186e-07

======================================================================
测试: GeGLU
======================================================================
输入形状: (2, 4, 128)
✓ GeGLU Forward: 误差=5.960464e-08
✓ GeGLU Backward: 误差=1.192093e-07

...

======================================================================
测试汇总
======================================================================
SwiGLU               ✓ 通过
GeGLU                ✓ 通过
RMSNorm              ✓ 通过
LayerNorm            ✓ 通过
RoPE                 ✓ 通过
Softmax              ✓ 通过
CrossEntropy         ✓ 通过
KL Div               ✓ 通过
JSD                  ✓ 通过
======================================================================
总计: 9/9 通过
======================================================================
```

## 测试逻辑

每个算子的测试流程：

1. **Forward 测试**：
   - 使用 Liger kernel 计算输出
   - 使用 PyTorch 原生实现计算期望输出
   - 比较两者的误差

2. **Backward 测试**：
   - 使用 Liger kernel 计算梯度
   - 使用 PyTorch autograd 计算参考梯度
   - 比较两者的误差

## 单独测试某个算子

可以在 Python 中单独调用：

```python
from test_npu_ops import test_swiglu, test_geglu, test_rms_norm

# 只测试 SwiGLU
test_swiglu()

# 只测试 RMSNorm
test_rms_norm()
```

## 调试建议

如果某个算子测试失败：

1. **检查设备支持**：确认 NPU 支持该算子所需的操作
2. **检查 Triton 支持**：某些 NPU 可能不支持 Triton，需要用原生实现
3. **降低精度**：尝试使用 `float16` 代替 `float32`
4. **查看详细错误**：运行时会打印详细的异常信息
5. **调整容差**：某些 NPU 可能有不同的数值精度特性

## 依赖项

- PyTorch (支持 NPU 的版本)
- Triton (如果 NPU 支持)
- Liger-Kernel

## 常见问题

### Q: 提示找不到 NPU 设备？
A: 确保安装了支持 NPU 的 PyTorch 版本，并且 NPU 驱动正常。

### Q: Triton 相关错误？
A: 某些 NPU 可能不支持 Triton，需要用原生 PyTorch 算子替代。

### Q: 精度误差较大？
A: 不同硬件的浮点运算精度可能不同，可以适当调整 `TOLERANCE` 参数。

### Q: 某个算子一直失败？
A: 可能该算子的 Triton 实现不兼容 NPU，需要针对性适配。

## 联系和反馈

如有问题，请提供：
- NPU 型号和驱动版本
- PyTorch 版本
- 完整的错误日志
- 失败的具体算子名称

